{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98ae0b8-0542-46ce-b16e-ff1ed62ab257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ea3593-0277-41c9-ba3f-bc87f59342e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set train_args.dataloader_num_workers = 4\n",
      "Loading trainer for analysis – setting --do_eval=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output -> <pad> The m<unk>lbies wase wyst bograge; And the sliths and toms wy</s>\n",
      "================ End trainer sanity check ================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the character of the skull, which are relatively smooth and untutuous in the case of infant sutures. Unlike the sutures,\n",
      "to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "individual from the Southern Hemisphere to win the Winter Olympic relay gold medal, and was also part of the Australian Short Track team, which won gold\n",
      "individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "the same rights as men, and 75% agreed that they should be protected from discrimination. Among the other 15%, gay workers are no longer exp\n",
      "the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0522613525390625,\n",
       " 'eval_accuracy': 0.7386067708333334,\n",
       " 'bleu_score': 31.876985502984844,\n",
       " 'emb_cos_sim': 0.9062262773513794,\n",
       " 'eval_perplexity': 2.8641205867926014,\n",
       " 'eval_runtime': 3.8306,\n",
       " 'eval_samples_per_second': 130.527,\n",
       " 'eval_steps_per_second': 1.044}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import analyze_utils\n",
    "\n",
    "# https://wandb.ai/jack-morris/emb-inv-1/runs/ebb31d91810c4b62d2b55b5382e8c7ea/logs?workspace=user-jxmorris12\n",
    "checkpoint_folder = '/home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea'\n",
    "args_str = '--per_device_train_batch_size 128 --per_device_eval_batch_size 128 --max_seq_length 32 --model_name_or_path t5-base --embedder_model_name gtr_base --num_repeat_tokens 16 --embedder_no_grad True --exp_group_name mar17-baselines --learning_rate 0.0003 --freeze_strategy none --embedder_fake_with_zeros False --use_frozen_embeddings_as_input False --num_train_epochs 24 --max_eval_samples 500 --eval_steps 25000 --warmup_steps 100000 --bf16=1 --use_wandb=1'\n",
    "trainer = analyze_utils.load_trainer(checkpoint_folder, args_str)\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a012a4f1-6ea4-4ebe-8e6b-802eb88d1e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 156,   27,  130,    3,    9, 2354,  388, 4701, 4698, 1227,  115,  115,\n",
      "            9, 1227,  115,  115,    9, 1227,  115,  115,    9, 1227,  115,  115,\n",
      "            9, 1227,  115,  115,    9, 1227,  115,  115,    9,  146,   51,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad> If I were a rich man I would be a rich man sir b b b b b b b ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def encode(s: str) -> torch.Tensor:\n",
    "    t = trainer.embedder_tokenizer(s, return_tensors='pt')\n",
    "    print(t)\n",
    "    t = t.to(trainer.args.device)\n",
    "    with torch.no_grad():\n",
    "        e = trainer.call_embedding_model(\n",
    "            input_ids=t['input_ids'],\n",
    "            attention_mask=t['attention_mask'],\n",
    "        )\n",
    "    return e\n",
    "\n",
    "def decode(t: torch.Tensor) -> List[str]:\n",
    "    trainer.model.use_frozen_embeddings_as_input = True\n",
    "    decoded_ids = trainer.model.generate(\n",
    "        inputs={\n",
    "            'embedder_input_ids': torch.tensor([1]),\n",
    "            'embedder_attention_mask': torch.tensor([1]),\n",
    "            'frozen_embeddings':t,\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            'early_stopping': False, \n",
    "            'num_beams': 1, \n",
    "            'do_sample': False, \n",
    "            'max_length': 32\n",
    "        }\n",
    "    )\n",
    "    return trainer.embedder_tokenizer.batch_decode(decoded_ids)\n",
    "\n",
    "decode(encode(\"If I were a rich man Ya ba dibba dibba dibba dibba dibba dibba dum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f73e27-7b55-45a6-afbf-e16dc6fa4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2840,    19,   312, 22780,    29,  2549,    45,    58,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad> LeBron James from the state of Florida LeBron James is a basketballer who was born in the city of Topeka, Ohio and']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"Where is LeBron James from?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e5948266-97b1-4253-98a9-7c925d0b0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[2840,   19,  312, 5407, 2549,   45,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 363,   19,    8, 1784,   13, 1826,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad> The Head of the State of California, where the capital of the U.S. is James Lenbon, the capital of Lenbon, born']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encode(\"Where is Lebon James from?\")\n",
    "b = encode(\"What is the capital of California?\")\n",
    "\n",
    "decode( (a+b)/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e87c5153-29fa-409d-b9e6-e515cc4ed51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "eb6fb7d5-bd2b-4488-ad23-0c68d9be0fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 796.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 1.532\n",
      "cs(a,c) = tensor([0.7548], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "cs(b,c) = tensor([0.7569], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "c.norm(p=2) = tensor(104.0242, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "c = torch.randn_like(a, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.SGD(lr=.1, params=[c])\n",
    "cs = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "for step in tqdm.trange(1000):\n",
    "    cos_loss = 2 -  (cs(a,c) + cs(b,c))\n",
    "    norm_loss = max(a.norm(p=2), b.norm(p=2), c.norm(p=2)) / 100\n",
    "    loss = cos_loss + norm_loss\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(f'Final loss = {loss.item():.3f}')\n",
    "print('cs(a,c) =', cs(a,c))\n",
    "print('cs(b,c) =', cs(b,c))\n",
    "print('c.norm(p=2) =', c.norm(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f730c614-85a4-456b-9c8a-650411911aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7548], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "77cbd33b-3e31-41b5-bf51-d603b2c77216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7229], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca26e6c0-5993-49a2-b818-a475ae7206e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Lena Horne, California on the first-born child in California, Bell Laboon is where the state capital is from. Baby boom noise']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "728ecdb8-bb07-4e6b-bb59-4eb98e361124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(210.4552, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.norm(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba133b-b901-49c8-86be-8000c0ac179e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
